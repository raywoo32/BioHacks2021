{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BioHacks2021_WebScraper.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPvi+crzStgNep9JB1vVMMs"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"d86zy9Z7ePiB"},"source":["# Augmenting Covid19 Data for Longterm Care Homes\r\n","\r\n","This is my webscraper, which will scrape data related to:\r\n","1. Home-type, for profit status\r\n","2. Number of non-compliances in most recent annual inspection \r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"pExG1dfLL63a"},"source":["### Part 1: Scraping Home-type and accrediation data "]},{"cell_type":"code","metadata":{"id":"rAUESVEYeLPc","executionInfo":{"status":"ok","timestamp":1615687199514,"user_tz":300,"elapsed":710,"user":{"displayName":"Rachel Woo","photoUrl":"","userId":"01967864102579312528"}}},"source":["# Imports \r\n","# !pip install beautifulsoup4\r\n","import requests \r\n","import csv\r\n","from bs4 import BeautifulSoup\r\n","from google.colab import files\r\n"],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"id":"buOpm2UchZK_","executionInfo":{"status":"ok","timestamp":1615682783012,"user_tz":300,"elapsed":1422,"user":{"displayName":"Rachel Woo","photoUrl":"","userId":"01967864102579312528"}}},"source":["# Get initial data \r\n","\r\n","with open('HomeIDs.csv') as csv_file:\r\n","    reader = csv.reader(csv_file, delimiter=',')\r\n","    homeIDs = list(reader)[0]\r\n","    homeIDs.remove('')\r\n","    #print(len(homeIDs))\r\n","\r\n","#There are 514 LTC homes "],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"id":"swvRJFVOf4fQ"},"source":["# Webscraper 1: For profit status, and home type\r\n","# DO NOT RUN!!! \r\n","\r\n","url_part1 = \"http://publicreporting.ltchomes.net/en-ca/homeprofile.aspx?Home=\"\r\n","url_part3 = \"&tab=0\"\r\n","\r\n","rows = []\r\n","notParsed = []\r\n","for id in homeIDs:\r\n","  print(\"Working on\", id)\r\n","  URL = url_part1 + id + url_part3\r\n","  page = requests.get(URL)\r\n","  #Not successful query\r\n","  if page.status_code != 200:\r\n","    print(\"Error fetching page\", id)\r\n","    notParsed.append(id)\r\n","  #Successful query\r\n","  else:\r\n","    soup = BeautifulSoup(page.content, 'html.parser')\r\n","    all = soup.find_all(class_=\"Profilerow_col2\")\r\n","    if (all == []):\r\n","      print(\"Error wt idn page\", id)\r\n","      notParsed.append(id)\r\n","    else:\r\n","      homeType = all[5].getText()\r\n","      ResCouncil = all[8].getText()\r\n","      FamCouncil = all[9].getText()\r\n","      Accreditation = all[10].getText()\r\n","      row = [id, homeType, ResCouncil, FamCouncil, Accreditation]\r\n","      print(row)\r\n","      rows.append(row)\r\n","\r\n","print(len(notParsed), notParsed)\r\n","print(len(rows))\r\n","#page = requests.get(URL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5eMJIA_jo-Eq"},"source":["# Save Webscraper 1 data to csv and download \r\n","\r\n","#LTC_accreditation\r\n","with open('test.csv', 'a') as f:\r\n","    writer = csv.writer(f)\r\n","    writer.writerow([\"LTCH_Num\", \"homeType\", \"ResCouncil\", \"FamCouncil\", \"Accreditation\"])\r\n","    for row in rows:\r\n","      writer.writerow(row)\r\n","\r\n","files.download('test.csv') \r\n","print(notParsed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6rSMcJrmvUTq"},"source":["# Manually looked up codes for codes that didn't work \r\n","\r\n","nonParsed = ['2983', '3052', '956', '2985', '2986', '2987', '958', '2988', '925',  '922',  '907', '2992', '3023', '3029', '3050', '3034', '2993', '3031', '2995', '901', '2996', '2997', '3035', '965', '3000', '3016', '3002', '3026', '3003', '3004', '909', '2990', '962', '3006', '3005', '2991', '3033', '3013', '3014', '943', '3009', '3028', '3012', '3011', '3015', '3051', 'M633', '995', '3025', '3021', '3017', '3001', '963', '3018', '3020']\r\n","toParse =   ['C501', '2647', '0956','C506', 'C507', 'C508', '0958','C516', '0925', '0922', '0907', 'C524','C593', 'C601', 'M542', 'C609', 'C525', 'C603', 'C530', '0901','C532', 'C533', 'C606', '0965','C540', 'C571', 'C543', 'C596', 'C546', 'C547', '0909','C522', '0962','C554', 'C553', 'C523', 'C608', 'C567', 'C568', '0943','C558', 'C599', 'C565', 'C564', 'C569', '1002', 'C521', '0995','C595', 'C579', 'C573', 'C542', '0963', 'C574', 'C577']\r\n","\r\n","idDict = {}\r\n","for i in range(len(toParse)):\r\n","  idDict[toParse[i]] = nonParsed[i]\r\n","\r\n","\r\n","url_part1 = \"http://publicreporting.ltchomes.net/en-ca/homeprofile.aspx?Home=\"\r\n","url_part3 = \"&tab=0\"\r\n","rows = []\r\n","notParsed = []\r\n","for id in toParse:\r\n","  print(\"Working on\", id)\r\n","  URL = url_part1 + id + url_part3\r\n","  page = requests.get(URL)\r\n","  #Not successful query\r\n","  if page.status_code != 200:\r\n","    print(\"Error fetching page\", id)\r\n","    notParsed.append(id)\r\n","  #Successful query\r\n","  else:\r\n","    soup = BeautifulSoup(page.content, 'html.parser')\r\n","    all = soup.find_all(class_=\"Profilerow_col2\")\r\n","    if (all == []):\r\n","      print(\"Error wt idn page\", id)\r\n","      notParsed.append(id)\r\n","    else:\r\n","      homeType = all[5].getText()\r\n","      ResCouncil = all[8].getText()\r\n","      FamCouncil = all[9].getText()\r\n","      Accreditation = all[10].getText()\r\n","      id = idDict[id]\r\n","      row = [id, homeType, ResCouncil, FamCouncil, Accreditation]\r\n","      print(row)\r\n","      rows.append(row)\r\n","\r\n","#LTC_accreditation\r\n","with open('part2.csv', 'a') as f:\r\n","    writer = csv.writer(f)\r\n","    #writer.writerow([\"LTCH_Num\", \"homeType\", \"ResCouncil\", \"FamCouncil\", \"Accreditation\"])\r\n","    for row in rows:\r\n","      writer.writerow(row)\r\n","\r\n","files.download('part2.csv') \r\n","print(notParsed)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"97U41wZGL0b9"},"source":["### Part 2: Scraping Non-compliance Data"]},{"cell_type":"code","metadata":{"id":"odDLvpIsG_ms"},"source":["# NON COMPLIANCE \r\n","\r\n","nonParsed = ['2983', '3052', '956', '2985', '2986', '2987', '958', '2988', '925',  '922',  '907', '2992', '3023', '3029', '3050', '3034', '2993', '3031', '2995', '901', '2996', '2997', '3035', '965', '3000', '3016', '3002', '3026', '3003', '3004', '909', '2990', '962', '3006', '3005', '2991', '3033', '3013', '3014', '943', '3009', '3028', '3012', '3011', '3015', '3051', 'M633', '995', '3025', '3021', '3017', '3001', '963', '3018', '3020']\r\n","toParse =   ['C501', '2647', '0956','C506', 'C507', 'C508', '0958','C516', '0925', '0922', '0907', 'C524','C593', 'C601', 'M542', 'C609', 'C525', 'C603', 'C530', '0901','C532', 'C533', 'C606', '0965','C540', 'C571', 'C543', 'C596', 'C546', 'C547', '0909','C522', '0962','C554', 'C553', 'C523', 'C608', 'C567', 'C568', '0943','C558', 'C599', 'C565', 'C564', 'C569', '1002', 'C521', '0995','C595', 'C579', 'C573', 'C542', '0963', 'C574', 'C577']\r\n","\r\n","\r\n","url_part1 = \"https://apps.mohltc.ca/ltchomes/detail.php?id=\"\r\n","url_part3 = \"&lang=en\"\r\n","rows = []\r\n","notParsed = []\r\n","for id in homeIDs:\r\n","  print(\"Working on\", id)\r\n","  URL = url_part1 + id + url_part3\r\n","  page = requests.get(URL)\r\n","  #Not successful query\r\n","  if page.status_code != 200:\r\n","    print(\"Error fetching page\", id)\r\n","    notParsed.append(id)\r\n","  #Successful query\r\n","  soup = BeautifulSoup(page.content, 'html.parser')\r\n","  all = soup.find_all('td')\r\n","  if (all == []):\r\n","    print(\"Error wt idn page\", id)\r\n","    notParsed.append(id)\r\n","  else:\r\n","    an_order = all[0].getText().split(\" \")[0]\r\n","    an_nc = all[2].getText().split(\" \")[0]\r\n","    tar_order = all[4].getText().split(\" \")[0]\r\n","    tar_nc = all[6].getText().split(\" \")[0]\r\n","    tar_num = all[8].getText()\r\n","    row = [id, an_order, an_nc, tar_order, tar_nc, tar_num]\r\n","    print(row)\r\n","    rows.append(row)\r\n","  \r\n","\r\n","print(len(notParsed), notParsed)\r\n","print(len(rows))\r\n","\r\n","#LTC_accreditation\r\n","# with open('compliance1.csv', 'a') as f:\r\n","#     writer = csv.writer(f)\r\n","#     #writer.writerow([\"LTCH_Num\", \"homeType\", \"ResCouncil\", \"FamCouncil\", \"Accreditation\"])\r\n","#     for row in rows:\r\n","#       writer.writerow(row)\r\n","\r\n","# files.download('compliance1.csv') \r\n","# print(notParsed)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hEpys5ONODag"},"source":["# Scratch \r\n","\r\n","#LTC_accreditation\r\n","columns = [\"LTCH_Num\", \"Annual_Orders\", \"Annual_Non-compliance\", \"Targeted_Inspect_Num\", \"Targeted_Orders\", \"Targeted_Non-compliance\"]\r\n","with open('nonCompliance.csv', 'a') as f:\r\n","    writer = csv.writer(f)\r\n","    writer.writerow(columns)\r\n","    for row in rows:\r\n","      writer.writerow(row)\r\n","\r\n","#files.download('nonCompliance.csv') \r\n","print(notParsed)\r\n","\r\n","notParsed = ['3052', '956', '2930', '2620', '958', '925', '922', '907', '1041', '2513', '2706', '2995', '901', '2974', '965', '2211', '3026', '909', '2693', '962', '2805', '2629', '943', '1135', '2762', '3051', '995', '963', '2803']\r\n","\r\n","#Data is incomplete, manual review was conducted for all "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sayAoS81jeEL"},"source":["### References: \r\n","1. https://realpython.com/beautiful-soup-web-scraper-python/ \r\n","2. https://www.scrapingbee.com/blog/python-web-scraping-beautiful-soup/"]}]}